{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Hands-on : BERT\n",
    "* We will use the BERT model for Question and Answer\n",
    "* How smart is BERT ? \n",
    " * Is it open to Adverserial attacks ?\n",
    " * Does it do well with the Winograd Schemas ?\n",
    "* To Do After the lab:\n",
    " * Try with different datasets\n",
    " * Try other tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch ver : 1.4.0\n",
      "TorchText Ver : 0.5.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"torch ver :\",torch.__version__)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#\n",
    "# pip install https://github.com/pytorch/text/archive/master.zip\n",
    "import torchtext\n",
    "print(F'TorchText Ver : {torchtext.__version__}')\n",
    "from torchtext import data\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from torchsummary import summary\n",
    "# pip install torchsummary\n",
    "#\n",
    "import numpy as np\n",
    "import datetime # use datetime.datetime.now() prints in a nice hh:mm:ss.nn forma\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda : F\n"
     ]
    }
   ],
   "source": [
    "is_cuda = False\n",
    "if torch.cuda.is_available():\n",
    "    is_cuda = True\n",
    "print('Cuda : {:.1s}'.format(str(is_cuda)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huggingface Transformers\n",
    "# pip install transformers\n",
    "# Might need rust compiler\n",
    "# curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n",
    "# need Cargo's bin directory ($HOME/.cargo/bin) in your PATH environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of BERT models from Huggingface https://huggingface.co/transformers/pretrained_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0224 18:43:55.499215 4688090560 file_utils.py:41] PyTorch version 1.4.0 available.\n",
      "/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "I0224 18:43:59.396711 4688090560 file_utils.py:57] TensorFlow version 2.0.0-beta1 available.\n",
      "I0224 18:44:01.084348 4688090560 tokenization_utils.py:484] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/ksankar/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0224 18:44:01.554395 4688090560 configuration_utils.py:254] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /Users/ksankar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0224 18:44:01.556943 4688090560 configuration_utils.py:290] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0224 18:44:01.932245 4688090560 modeling_utils.py:458] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /Users/ksankar/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "bert_tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = transformers.BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n",
      "['[CLS]', 'we', 'like', 'unicorn', '##s', 'because', 'they', '[SEP]']\n",
      "['[CLS]', 'here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.', '[SEP]']\n",
      "['[CLS]', 'i', 'don', \"'\", 't', 'know', 'if', 'it', 'is', 'an', 'em', '##bed', '##ding', ',', 'em', '##bed', '##dable', 'or', 'can', 'be', 'embedded', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(bert_tokenizer.vocab_size)\n",
    "input_text = bert_tokenizer.encode(\"We like Unicorns because they\")\n",
    "print(bert_tokenizer.convert_ids_to_tokens(input_text))\n",
    "input_text = bert_tokenizer.encode(\"Here is the sentence I want embeddings for.\")\n",
    "print(bert_tokenizer.convert_ids_to_tokens(input_text))\n",
    "input_text = bert_tokenizer.encode(\"I don't know if it is an embedding, embeddable or can be embedded\")\n",
    "print(bert_tokenizer.convert_ids_to_tokens(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0224 18:44:13.001343 4688090560 configuration_utils.py:254] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /Users/ksankar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0224 18:44:13.003283 4688090560 configuration_utils.py:290] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0224 18:44:13.399081 4688090560 modeling_utils.py:458] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /Users/ksankar/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I0224 18:44:16.892946 4688090560 modeling_utils.py:543] Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
      "I0224 18:44:16.895022 4688090560 modeling_utils.py:549] Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8])\n",
      "tensor([[  101,  7592,  1010,  2026,  3899,  2003, 10140,   102]])\n",
      "(tensor(8.3524, grad_fn=<NllLossBackward>), tensor([[[ -8.3153,  -8.2307,  -8.2094,  ...,  -7.8594,  -7.9515,  -4.8554],\n",
      "         [ -8.8652,  -8.8603,  -8.9235,  ...,  -8.8525,  -8.7731,  -6.2125],\n",
      "         [-15.7144, -15.8929, -16.0055,  ..., -13.7756, -12.4860, -11.9257],\n",
      "         ...,\n",
      "         [-14.4819, -14.6738, -14.7835,  ..., -13.5051, -11.9443, -10.7282],\n",
      "         [-11.0767, -11.2094, -11.4593,  ..., -11.1133, -10.9400,  -4.2686],\n",
      "         [-11.7574, -11.8792, -11.5958,  ..., -10.0051, -10.3031, -10.0536]]],\n",
      "       grad_fn=<AddBackward0>))\n",
      "tensor(8.3524, grad_fn=<NllLossBackward>)\n",
      "torch.Size([1, 8, 30522])\n"
     ]
    }
   ],
   "source": [
    "# Just try a LM model as a test. We won'ut use this here. \n",
    "# But as a homework exerrcise, would be interesting to try and compare with GPT-2\n",
    "model = transformers.BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "input_ids = torch.tensor(bert_tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)\n",
    "    # Batch size 1\n",
    "print(input_ids.size())\n",
    "print(input_ids)\n",
    "outputs = model(input_ids, masked_lm_labels=input_ids)\n",
    "\n",
    "print(outputs)\n",
    "\n",
    "loss, prediction_scores = outputs[:2]\n",
    "print(loss)\n",
    "print(prediction_scores.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us try Question-Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0224 18:44:22.960361 4688090560 tokenization_utils.py:484] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/ksankar/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0224 18:44:23.427194 4688090560 configuration_utils.py:254] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-config.json from cache at /Users/ksankar/.cache/torch/transformers/00bf8b18639562831ff9d22825e646f00f34a6976c6ed0ed1c4b35d334bb7a51.84dc10193e066c2b505f965e0ba0751ef60dba0afccfad2e23acd97eb7feec5e\n",
      "I0224 18:44:23.429436 4688090560 configuration_utils.py:290] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0224 18:44:23.810264 4688090560 modeling_utils.py:458] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-pytorch_model.bin from cache at /Users/ksankar/.cache/torch/transformers/ca2ac20761877486c1e2204d99653106b9adacf9a5eb18ec71b41d2dbef42103.2db7ae79c41a184c87600faabafa1369db2b16457723fd154ca3b436c4172807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a nice puppet\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_l_qa_model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "def get_answer_from_bert(model,question,passage):\n",
    "    input_ids = tokenizer.encode(question, passage)\n",
    "    token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]\n",
    "    start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))\n",
    "\n",
    "    all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    answer = ' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1])\n",
    "    return (answer)\n",
    "\n",
    "question, passage = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
    "answer = get_answer_from_bert(bert_l_qa_model,question,passage)\n",
    "assert answer == \"a nice puppet\"\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us try some adverserial examples\n",
    "* Adversarial Examples for Evaluating Reading Comprehension Systems, Robin Jia, Percy Liang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](BERT-Adv-01.png \"Adverserial Example - Super Bowl 50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "john el ##way\n"
     ]
    }
   ],
   "source": [
    "passage = \"Peyton Manning became the first quarterback ever to lead \\\n",
    "two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play \\\n",
    "in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super\\\n",
    "Bowl XXXIII at age 38 and is currently Denver’s Executive Vice President of Football Operations and General Manager\"\n",
    "question = \"What is the name of the quarterback who was 38 in Super Bowl XXXIII?\"\n",
    "\n",
    "answer = get_answer_from_bert(bert_l_qa_model,question,passage)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "john el ##way\n"
     ]
    }
   ],
   "source": [
    "passage = \"Peyton Manning became the first quarterback ever to lead \\\n",
    "two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play \\\n",
    "in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super\\\n",
    "Bowl XXXIII at age 38 and is currently Denver’s Executive Vice President of Football Operations and General Manager.\\\n",
    "Quarterback Jeff Dean had jersey number 37 in Champ Bowl XXXIV\"\n",
    "\n",
    "question = \"What is the name of the quarterback who was 38 in Super Bowl XXXIII?\"\n",
    "\n",
    "answer = get_answer_from_bert(bert_l_qa_model,question,passage)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](BERT-Adv-02.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prague\n"
     ]
    }
   ],
   "source": [
    "passage = \"In January 1880, two of Tesla's uncles put together enough money to help him leave \\\n",
    "Gospić for Prague where he was to study. Unfortunately, he arrived too late to enroll at \\\n",
    "Charles-Ferdinand University; he never studied Greek, a required subject; and he was illiterate in \\\n",
    "Czech, another required subject. Tesla did, however, attend lectures at the university, although, as an \\\n",
    "auditor, he did not receive grades for the courses.\"\n",
    "\n",
    "question = \"What city did Tesla move to in 1880?\"\n",
    "\n",
    "answer = get_answer_from_bert(bert_l_qa_model,question,passage)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prague\n"
     ]
    }
   ],
   "source": [
    "passage = \"In January 1880, two of Tesla's uncles put together enough money to help him leave \\\n",
    "Gospić for Prague where he was to study. Unfortunately, he arrived too late to enroll at \\\n",
    "Charles-Ferdinand University; he never studied Greek, a required subject; and he was illiterate in \\\n",
    "Czech, another required subject. Tesla did, however, attend lectures at the university, although, as an \\\n",
    "auditor, he did not receive grades for the courses. Tadakatsu moved to the city of Chicago in 1881.\"\n",
    "\n",
    "question = \"What city did Tesla move to in 1880?\"\n",
    "\n",
    "answer = get_answer_from_bert(bert_l_qa_model,question,passage)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Good News, the attacks do not work !_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about the Winograd Schemas ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city council ##men\n"
     ]
    }
   ],
   "source": [
    "passage = \"The city councilmen refused the demonstrators a permit because they feared violence\"\n",
    "question = \"Who feared violence?\"\n",
    "answer = get_answer_from_bert(bert_l_qa_model,question,passage)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demonstrators\n"
     ]
    }
   ],
   "source": [
    "passage = \"The city councilmen refused the demonstrators a permit because they advocated violence\"\n",
    "question = \"Who advocated violence?\"\n",
    "answer = get_answer_from_bert(bert_l_qa_model,question,passage)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the worm\n"
     ]
    }
   ],
   "source": [
    "# Ref: Using Answer Set Programming for Commonsense Reasoning in the Winograd Schema Challenge \n",
    "#   https://arxiv.org/abs/1907.11112\n",
    "passage = \"The fish ate the worm. It was tasty\"\n",
    "question = \"What was tasty?\"\n",
    "answer = get_answer_from_bert(bert_l_qa_model,question,passage)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### May be this is why !\n",
    "* Ref : A Surprisingly Robust Trick for the Winograd Schema Challenge https://arxiv.org/abs/1905.06290"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the trophy\n"
     ]
    }
   ],
   "source": [
    "passage = \"The trophy didn’t fit into the suitcase because it was too large\"\n",
    "question = \"What was too large?\"\n",
    "answer = get_answer_from_bert(bert_l_qa_model,question,passage)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### May be not !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the trophy\n"
     ]
    }
   ],
   "source": [
    "passage = \"The trophy didn’t fit into the suitcase because it was too tiny\"\n",
    "question = \"What was too tiny?\"\n",
    "answer = get_answer_from_bert(bert_l_qa_model,question,passage)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WIP area\n",
    "### To experiment with different things "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* summary() doesn't work well\n",
    "* you can print(model) to get an idea about the layers\n",
    "* The model_describe() below works reasonably, but the total parameters doesn't look correct\n",
    "* Let me know if you were able to get this to work correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def model_describe(model):\n",
    "    print(\"model_summary\")\n",
    "    print()\n",
    "    print(F'Layer_name {\"\":45s} Size {\"\":25s} Number of Parameters')\n",
    "    print(\"=\"*110)\n",
    "    total_params = 0\n",
    "    for param_tensor in model.state_dict():\n",
    "        print(F'Layer {param_tensor:50s} {str(model.state_dict()[param_tensor].size()):30s} '\n",
    "              F'elements : {torch.numel(model.state_dict()[param_tensor]):,}')\n",
    "        total_params += torch.numel(model.state_dict()[param_tensor])\n",
    "    # total_params = sum(p.numel() for p in model.parameters()) # sum number of elements\n",
    "    print(\"=\"*110)\n",
    "    print(f\"Total Params:{total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_summary\n",
      "\n",
      "Layer_name                                               Size                           Number of Parameters\n",
      "==============================================================================================================\n",
      "Layer bert.embeddings.word_embeddings.weight             torch.Size([30522, 1024])      elements : 31,254,528\n",
      "Layer bert.embeddings.position_embeddings.weight         torch.Size([512, 1024])        elements : 524,288\n",
      "Layer bert.embeddings.token_type_embeddings.weight       torch.Size([2, 1024])          elements : 2,048\n",
      "Layer bert.embeddings.LayerNorm.weight                   torch.Size([1024])             elements : 1,024\n",
      "Layer bert.embeddings.LayerNorm.bias                     torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.0.attention.self.query.weight   torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.0.attention.self.query.bias     torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.0.attention.self.key.weight     torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.0.attention.self.key.bias       torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.0.attention.self.value.weight   torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.0.attention.self.value.bias     torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.0.attention.output.dense.weight torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.0.attention.output.dense.bias   torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.0.intermediate.dense.weight     torch.Size([4096, 1024])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.0.intermediate.dense.bias       torch.Size([4096])             elements : 4,096\n",
      "Layer bert.encoder.layer.0.output.dense.weight           torch.Size([1024, 4096])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.0.output.dense.bias             torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.0.output.LayerNorm.weight       torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.0.output.LayerNorm.bias         torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.1.attention.self.query.weight   torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.1.attention.self.query.bias     torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.1.attention.self.key.weight     torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.1.attention.self.key.bias       torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.1.attention.self.value.weight   torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.1.attention.self.value.bias     torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.1.attention.output.dense.weight torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.1.attention.output.dense.bias   torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.1.intermediate.dense.weight     torch.Size([4096, 1024])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.1.intermediate.dense.bias       torch.Size([4096])             elements : 4,096\n",
      "Layer bert.encoder.layer.1.output.dense.weight           torch.Size([1024, 4096])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.1.output.dense.bias             torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.1.output.LayerNorm.weight       torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.1.output.LayerNorm.bias         torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.2.attention.self.query.weight   torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.2.attention.self.query.bias     torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.2.attention.self.key.weight     torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.2.attention.self.key.bias       torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.2.attention.self.value.weight   torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.2.attention.self.value.bias     torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.2.attention.output.dense.weight torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.2.attention.output.dense.bias   torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.2.intermediate.dense.weight     torch.Size([4096, 1024])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.2.intermediate.dense.bias       torch.Size([4096])             elements : 4,096\n",
      "Layer bert.encoder.layer.2.output.dense.weight           torch.Size([1024, 4096])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.2.output.dense.bias             torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.2.output.LayerNorm.weight       torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.2.output.LayerNorm.bias         torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.3.attention.self.query.weight   torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.3.attention.self.query.bias     torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.3.attention.self.key.weight     torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.3.attention.self.key.bias       torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.3.attention.self.value.weight   torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.3.attention.self.value.bias     torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.3.attention.output.dense.weight torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.3.attention.output.dense.bias   torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.3.intermediate.dense.weight     torch.Size([4096, 1024])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.3.intermediate.dense.bias       torch.Size([4096])             elements : 4,096\n",
      "Layer bert.encoder.layer.3.output.dense.weight           torch.Size([1024, 4096])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.3.output.dense.bias             torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.3.output.LayerNorm.weight       torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.3.output.LayerNorm.bias         torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.4.attention.self.query.weight   torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.4.attention.self.query.bias     torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.4.attention.self.key.weight     torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.4.attention.self.key.bias       torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.4.attention.self.value.weight   torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.4.attention.self.value.bias     torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.4.attention.output.dense.weight torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.4.attention.output.dense.bias   torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.4.intermediate.dense.weight     torch.Size([4096, 1024])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.4.intermediate.dense.bias       torch.Size([4096])             elements : 4,096\n",
      "Layer bert.encoder.layer.4.output.dense.weight           torch.Size([1024, 4096])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.4.output.dense.bias             torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.4.output.LayerNorm.weight       torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.4.output.LayerNorm.bias         torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.5.attention.self.query.weight   torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.5.attention.self.query.bias     torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.5.attention.self.key.weight     torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.5.attention.self.key.bias       torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.5.attention.self.value.weight   torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.5.attention.self.value.bias     torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.5.attention.output.dense.weight torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.5.attention.output.dense.bias   torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.5.intermediate.dense.weight     torch.Size([4096, 1024])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.5.intermediate.dense.bias       torch.Size([4096])             elements : 4,096\n",
      "Layer bert.encoder.layer.5.output.dense.weight           torch.Size([1024, 4096])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.5.output.dense.bias             torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.5.output.LayerNorm.weight       torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.5.output.LayerNorm.bias         torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.6.attention.self.query.weight   torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.6.attention.self.query.bias     torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.6.attention.self.key.weight     torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.6.attention.self.key.bias       torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.6.attention.self.value.weight   torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.6.attention.self.value.bias     torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.6.attention.output.dense.weight torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.6.attention.output.dense.bias   torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.6.intermediate.dense.weight     torch.Size([4096, 1024])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.6.intermediate.dense.bias       torch.Size([4096])             elements : 4,096\n",
      "Layer bert.encoder.layer.6.output.dense.weight           torch.Size([1024, 4096])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.6.output.dense.bias             torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.6.output.LayerNorm.weight       torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.6.output.LayerNorm.bias         torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.7.attention.self.query.weight   torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.7.attention.self.query.bias     torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.7.attention.self.key.weight     torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.7.attention.self.key.bias       torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.7.attention.self.value.weight   torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.7.attention.self.value.bias     torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.7.attention.output.dense.weight torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.7.attention.output.dense.bias   torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.7.intermediate.dense.weight     torch.Size([4096, 1024])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.7.intermediate.dense.bias       torch.Size([4096])             elements : 4,096\n",
      "Layer bert.encoder.layer.7.output.dense.weight           torch.Size([1024, 4096])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.7.output.dense.bias             torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.7.output.LayerNorm.weight       torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.7.output.LayerNorm.bias         torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.8.attention.self.query.weight   torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.8.attention.self.query.bias     torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.8.attention.self.key.weight     torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.8.attention.self.key.bias       torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.8.attention.self.value.weight   torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.8.attention.self.value.bias     torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.8.attention.output.dense.weight torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.8.attention.output.dense.bias   torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.8.intermediate.dense.weight     torch.Size([4096, 1024])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.8.intermediate.dense.bias       torch.Size([4096])             elements : 4,096\n",
      "Layer bert.encoder.layer.8.output.dense.weight           torch.Size([1024, 4096])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.8.output.dense.bias             torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.8.output.LayerNorm.weight       torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.8.output.LayerNorm.bias         torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.9.attention.self.query.weight   torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.9.attention.self.query.bias     torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.9.attention.self.key.weight     torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.9.attention.self.key.bias       torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.9.attention.self.value.weight   torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.9.attention.self.value.bias     torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.9.attention.output.dense.weight torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.9.attention.output.dense.bias   torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.9.intermediate.dense.weight     torch.Size([4096, 1024])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.9.intermediate.dense.bias       torch.Size([4096])             elements : 4,096\n",
      "Layer bert.encoder.layer.9.output.dense.weight           torch.Size([1024, 4096])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.9.output.dense.bias             torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.9.output.LayerNorm.weight       torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.9.output.LayerNorm.bias         torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.10.attention.self.query.weight  torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.10.attention.self.query.bias    torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.10.attention.self.key.weight    torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.10.attention.self.key.bias      torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.10.attention.self.value.weight  torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.10.attention.self.value.bias    torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.10.attention.output.dense.weight torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.10.attention.output.dense.bias  torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.10.intermediate.dense.weight    torch.Size([4096, 1024])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.10.intermediate.dense.bias      torch.Size([4096])             elements : 4,096\n",
      "Layer bert.encoder.layer.10.output.dense.weight          torch.Size([1024, 4096])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.10.output.dense.bias            torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.10.output.LayerNorm.weight      torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.10.output.LayerNorm.bias        torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.11.attention.self.query.weight  torch.Size([1024, 1024])       elements : 1,048,576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer bert.encoder.layer.11.attention.self.query.bias    torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.11.attention.self.key.weight    torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.11.attention.self.key.bias      torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.11.attention.self.value.weight  torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.11.attention.self.value.bias    torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.11.attention.output.dense.weight torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.11.attention.output.dense.bias  torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.11.intermediate.dense.weight    torch.Size([4096, 1024])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.11.intermediate.dense.bias      torch.Size([4096])             elements : 4,096\n",
      "Layer bert.encoder.layer.11.output.dense.weight          torch.Size([1024, 4096])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.11.output.dense.bias            torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.11.output.LayerNorm.weight      torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.11.output.LayerNorm.bias        torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.12.attention.self.query.weight  torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.12.attention.self.query.bias    torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.12.attention.self.key.weight    torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.12.attention.self.key.bias      torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.12.attention.self.value.weight  torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.12.attention.self.value.bias    torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.12.attention.output.dense.weight torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.12.attention.output.dense.bias  torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.12.attention.output.LayerNorm.weight torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.12.attention.output.LayerNorm.bias torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.12.intermediate.dense.weight    torch.Size([4096, 1024])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.12.intermediate.dense.bias      torch.Size([4096])             elements : 4,096\n",
      "Layer bert.encoder.layer.12.output.dense.weight          torch.Size([1024, 4096])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.12.output.dense.bias            torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.12.output.LayerNorm.weight      torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.12.output.LayerNorm.bias        torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.13.attention.self.query.weight  torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.13.attention.self.query.bias    torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.13.attention.self.key.weight    torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.13.attention.self.key.bias      torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.13.attention.self.value.weight  torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.13.attention.self.value.bias    torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.13.attention.output.dense.weight torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.13.attention.output.dense.bias  torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.13.attention.output.LayerNorm.weight torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.13.attention.output.LayerNorm.bias torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.13.intermediate.dense.weight    torch.Size([4096, 1024])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.13.intermediate.dense.bias      torch.Size([4096])             elements : 4,096\n",
      "Layer bert.encoder.layer.13.output.dense.weight          torch.Size([1024, 4096])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.13.output.dense.bias            torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.13.output.LayerNorm.weight      torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.13.output.LayerNorm.bias        torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.14.attention.self.query.weight  torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.14.attention.self.query.bias    torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.14.attention.self.key.weight    torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.14.attention.self.key.bias      torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.14.attention.self.value.weight  torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.14.attention.self.value.bias    torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.14.attention.output.dense.weight torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.14.attention.output.dense.bias  torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.14.attention.output.LayerNorm.weight torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.14.attention.output.LayerNorm.bias torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.14.intermediate.dense.weight    torch.Size([4096, 1024])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.14.intermediate.dense.bias      torch.Size([4096])             elements : 4,096\n",
      "Layer bert.encoder.layer.14.output.dense.weight          torch.Size([1024, 4096])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.14.output.dense.bias            torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.14.output.LayerNorm.weight      torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.14.output.LayerNorm.bias        torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.15.attention.self.query.weight  torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.15.attention.self.query.bias    torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.15.attention.self.key.weight    torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.15.attention.self.key.bias      torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.15.attention.self.value.weight  torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.15.attention.self.value.bias    torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.15.attention.output.dense.weight torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.15.attention.output.dense.bias  torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.15.attention.output.LayerNorm.weight torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.15.attention.output.LayerNorm.bias torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.15.intermediate.dense.weight    torch.Size([4096, 1024])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.15.intermediate.dense.bias      torch.Size([4096])             elements : 4,096\n",
      "Layer bert.encoder.layer.15.output.dense.weight          torch.Size([1024, 4096])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.15.output.dense.bias            torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.15.output.LayerNorm.weight      torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.15.output.LayerNorm.bias        torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.16.attention.self.query.weight  torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.16.attention.self.query.bias    torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.16.attention.self.key.weight    torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.16.attention.self.key.bias      torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.16.attention.self.value.weight  torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.16.attention.self.value.bias    torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.16.attention.output.dense.weight torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.16.attention.output.dense.bias  torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.16.attention.output.LayerNorm.weight torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.16.attention.output.LayerNorm.bias torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.16.intermediate.dense.weight    torch.Size([4096, 1024])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.16.intermediate.dense.bias      torch.Size([4096])             elements : 4,096\n",
      "Layer bert.encoder.layer.16.output.dense.weight          torch.Size([1024, 4096])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.16.output.dense.bias            torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.16.output.LayerNorm.weight      torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.16.output.LayerNorm.bias        torch.Size([1024])             elements : 1,024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer bert.encoder.layer.17.attention.self.query.weight  torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.17.attention.self.query.bias    torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.17.attention.self.key.weight    torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.17.attention.self.key.bias      torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.17.attention.self.value.weight  torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.17.attention.self.value.bias    torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.17.attention.output.dense.weight torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.17.attention.output.dense.bias  torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.17.attention.output.LayerNorm.weight torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.17.attention.output.LayerNorm.bias torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.17.intermediate.dense.weight    torch.Size([4096, 1024])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.17.intermediate.dense.bias      torch.Size([4096])             elements : 4,096\n",
      "Layer bert.encoder.layer.17.output.dense.weight          torch.Size([1024, 4096])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.17.output.dense.bias            torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.17.output.LayerNorm.weight      torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.17.output.LayerNorm.bias        torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.18.attention.self.query.weight  torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.18.attention.self.query.bias    torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.18.attention.self.key.weight    torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.18.attention.self.key.bias      torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.18.attention.self.value.weight  torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.18.attention.self.value.bias    torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.18.attention.output.dense.weight torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.18.attention.output.dense.bias  torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.18.attention.output.LayerNorm.weight torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.18.attention.output.LayerNorm.bias torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.18.intermediate.dense.weight    torch.Size([4096, 1024])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.18.intermediate.dense.bias      torch.Size([4096])             elements : 4,096\n",
      "Layer bert.encoder.layer.18.output.dense.weight          torch.Size([1024, 4096])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.18.output.dense.bias            torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.18.output.LayerNorm.weight      torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.18.output.LayerNorm.bias        torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.19.attention.self.query.weight  torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.19.attention.self.query.bias    torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.19.attention.self.key.weight    torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.19.attention.self.key.bias      torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.19.attention.self.value.weight  torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.19.attention.self.value.bias    torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.19.attention.output.dense.weight torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.19.attention.output.dense.bias  torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.19.attention.output.LayerNorm.weight torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.19.attention.output.LayerNorm.bias torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.19.intermediate.dense.weight    torch.Size([4096, 1024])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.19.intermediate.dense.bias      torch.Size([4096])             elements : 4,096\n",
      "Layer bert.encoder.layer.19.output.dense.weight          torch.Size([1024, 4096])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.19.output.dense.bias            torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.19.output.LayerNorm.weight      torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.19.output.LayerNorm.bias        torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.20.attention.self.query.weight  torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.20.attention.self.query.bias    torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.20.attention.self.key.weight    torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.20.attention.self.key.bias      torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.20.attention.self.value.weight  torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.20.attention.self.value.bias    torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.20.attention.output.dense.weight torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.20.attention.output.dense.bias  torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.20.attention.output.LayerNorm.weight torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.20.attention.output.LayerNorm.bias torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.20.intermediate.dense.weight    torch.Size([4096, 1024])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.20.intermediate.dense.bias      torch.Size([4096])             elements : 4,096\n",
      "Layer bert.encoder.layer.20.output.dense.weight          torch.Size([1024, 4096])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.20.output.dense.bias            torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.20.output.LayerNorm.weight      torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.20.output.LayerNorm.bias        torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.21.attention.self.query.weight  torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.21.attention.self.query.bias    torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.21.attention.self.key.weight    torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.21.attention.self.key.bias      torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.21.attention.self.value.weight  torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.21.attention.self.value.bias    torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.21.attention.output.dense.weight torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.21.attention.output.dense.bias  torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.21.attention.output.LayerNorm.weight torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.21.attention.output.LayerNorm.bias torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.21.intermediate.dense.weight    torch.Size([4096, 1024])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.21.intermediate.dense.bias      torch.Size([4096])             elements : 4,096\n",
      "Layer bert.encoder.layer.21.output.dense.weight          torch.Size([1024, 4096])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.21.output.dense.bias            torch.Size([1024])             elements : 1,024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer bert.encoder.layer.21.output.LayerNorm.weight      torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.21.output.LayerNorm.bias        torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.22.attention.self.query.weight  torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.22.attention.self.query.bias    torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.22.attention.self.key.weight    torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.22.attention.self.key.bias      torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.22.attention.self.value.weight  torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.22.attention.self.value.bias    torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.22.attention.output.dense.weight torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.22.attention.output.dense.bias  torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.22.attention.output.LayerNorm.weight torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.22.attention.output.LayerNorm.bias torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.22.intermediate.dense.weight    torch.Size([4096, 1024])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.22.intermediate.dense.bias      torch.Size([4096])             elements : 4,096\n",
      "Layer bert.encoder.layer.22.output.dense.weight          torch.Size([1024, 4096])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.22.output.dense.bias            torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.22.output.LayerNorm.weight      torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.22.output.LayerNorm.bias        torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.23.attention.self.query.weight  torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.23.attention.self.query.bias    torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.23.attention.self.key.weight    torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.23.attention.self.key.bias      torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.23.attention.self.value.weight  torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.23.attention.self.value.bias    torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.23.attention.output.dense.weight torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.encoder.layer.23.attention.output.dense.bias  torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.23.attention.output.LayerNorm.weight torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.23.attention.output.LayerNorm.bias torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.23.intermediate.dense.weight    torch.Size([4096, 1024])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.23.intermediate.dense.bias      torch.Size([4096])             elements : 4,096\n",
      "Layer bert.encoder.layer.23.output.dense.weight          torch.Size([1024, 4096])       elements : 4,194,304\n",
      "Layer bert.encoder.layer.23.output.dense.bias            torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.23.output.LayerNorm.weight      torch.Size([1024])             elements : 1,024\n",
      "Layer bert.encoder.layer.23.output.LayerNorm.bias        torch.Size([1024])             elements : 1,024\n",
      "Layer bert.pooler.dense.weight                           torch.Size([1024, 1024])       elements : 1,048,576\n",
      "Layer bert.pooler.dense.bias                             torch.Size([1024])             elements : 1,024\n",
      "Layer qa_outputs.weight                                  torch.Size([2, 1024])          elements : 2,048\n",
      "Layer qa_outputs.bias                                    torch.Size([2])                elements : 2\n",
      "==============================================================================================================\n",
      "Total Params:335,143,938\n"
     ]
    }
   ],
   "source": [
    "model_describe(bert_l_qa_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForQuestionAnswering(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 1024)\n",
      "      (token_type_embeddings): Embedding(2, 1024)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (12): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (13): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (14): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (15): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (16): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (17): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (18): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (19): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (20): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (21): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (22): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (23): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (qa_outputs): Linear(in_features=1024, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(bert_l_qa_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just adds a linear layer with Span-start and Span-end\n",
    "![](BERT-QA-Model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def model_summary(model): # This calculates wrong, may be it is correct - need to check\n",
    "  print(\"model_summary\")\n",
    "  print()\n",
    "  print(\"Layer_name\"+\"\\t\"*7+\"Number of Parameters\")\n",
    "  print(\"=\"*100)\n",
    "  model_parameters = [layer for layer in model.parameters() if layer.requires_grad]\n",
    "  layer_name = [child for child in model.children()]\n",
    "  j = 0\n",
    "  total_params = 0\n",
    "  print(\"\\t\"*10)\n",
    "  for i in layer_name:\n",
    "    print()\n",
    "    param = 0\n",
    "    try:\n",
    "      bias = (i.bias is not None)\n",
    "    except:\n",
    "      bias = False  \n",
    "    if not bias: # or is it bias ?\n",
    "      param =model_parameters[j].numel()+model_parameters[j+1].numel()\n",
    "      j = j+2\n",
    "    else:\n",
    "      param =model_parameters[j].numel()\n",
    "      j = j+1\n",
    "    print(str(i)+\"\\t\"*3+str(param))\n",
    "    total_params+=param\n",
    "  print(\"=\"*100)\n",
    "  print(f\"Total Params:{total_params}\")       \n",
    "\n",
    "# model_summary(bert_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handy code for testing elapsed time. It prints hh:mm:ss.nnn correctly. time.time() prints the total seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed - 0:02:25.006721\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "start_time = datetime.datetime.now()\n",
    "time.sleep(145)\n",
    "print(F'Elapsed - {datetime.datetime.now() - start_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _That's All Folks !_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
