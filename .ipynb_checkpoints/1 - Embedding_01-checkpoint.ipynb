{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Hands-on - Embedding : Differences, Similarities & Surprises !\n",
    "* We will look at embeddings from gensim,spacy, BERT and GPT2\n",
    "* Eventhough we haven't discussed BERT and GPT in detail, we can still see what the embeddings do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch ver : 1.4.0\n",
      "TorchText Ver : 0.5.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"torch ver :\",torch.__version__)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#\n",
    "# pip install https://github.com/pytorch/text/archive/master.zip\n",
    "import torchtext\n",
    "print(F'TorchText Ver : {torchtext.__version__}')\n",
    "from torchtext import data\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from torchsummary import summary\n",
    "# pip install torchsummary\n",
    "#\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda : F\n"
     ]
    }
   ],
   "source": [
    "is_cuda = False\n",
    "if torch.cuda.is_available():\n",
    "    is_cuda = True\n",
    "print('Cuda : {:.1s}'.format(str(is_cuda)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://www.shanelynn.ie/word-embeddings-in-python-with-spacy-and-gensim/\n",
    "# Ref: https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n",
    "#     #sphx-glr-auto-examples-tutorials-run-word2vec-py\n",
    "# pip install -U gensim\n",
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300') # 1662.8 MB ~2Gb ! Takes a long time. So commented out\n",
    "print(\"== Loaded ==\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x1a1c161ef0>\n",
      "(3000000, 300)\n",
      "gensim vocabulary size = 3,000,000 // model dimensionality = 300\n"
     ]
    }
   ],
   "source": [
    "print(wv)\n",
    "print(wv.vectors.shape)\n",
    "print(F'gensim vocabulary size = {wv.vectors.shape[0]:,} // model dimensionality = {wv.vectors.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U spacy\n",
    "# python -m spacy download en_core_web_md\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy vocabulary size = 1,340,242 // model dimensionality = 300\n"
     ]
    }
   ],
   "source": [
    "print(F'spacy vocabulary size = {nlp.vocab.length:,} // model dimensionality = {nlp.vocab.vectors_length}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us do some semantic computations using the word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    }
   ],
   "source": [
    "print(wv.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'car'\t'minivan'\t0.69\n",
      "'car'\t'bicycle'\t0.54\n",
      "'car'\t'airplane'\t0.42\n",
      "'car'\t'cereal'\t0.14\n",
      "'car'\t'communism'\t0.06\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    ('car', 'minivan'),   # a minivan is a kind of car\n",
    "    ('car', 'bicycle'),   # still a wheeled vehicle\n",
    "    ('car', 'airplane'),  # ok, no wheels, but still a vehicle\n",
    "    ('car', 'cereal'),    # ... and so on\n",
    "    ('car', 'communism'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192911148071),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431607246399),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377321243286133),\n",
       " ('kings', 0.5236844420433044),\n",
       " ('Queen_Consort', 0.5235945582389832),\n",
       " ('queens', 0.5181134343147278),\n",
       " ('sultan', 0.5098593235015869),\n",
       " ('monarchy', 0.5087411999702454)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Common king - man + woman\n",
    "wv.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "# Library - Books = Hall\n",
    "# Obama + Russia - USA = Putin\n",
    "# Human - Animal = Ethics\n",
    "# Ref ; http://byterot.blogspot.com/2015/06/five-crazy-abstractions-my-deep-learning-word2doc-model-just-did-NLP-gensim.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Public_Library', 0.38507434725761414),\n",
       " ('library', 0.3558593988418579),\n",
       " ('Historical_Museum', 0.331419438123703),\n",
       " ('Municipal_Building', 0.3273163437843323),\n",
       " ('Courthouse', 0.3168659210205078),\n",
       " ('Branch_Library', 0.3032086491584778),\n",
       " ('Museum', 0.2970375120639801),\n",
       " ('Historic_Courthouse', 0.2959327697753906),\n",
       " ('Annex_Building', 0.29544758796691895),\n",
       " ('Civic_Center', 0.28546392917633057)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(positive=['Library'], negative=['Books'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Medvedev', 0.673559308052063),\n",
       " ('Putin', 0.6472188234329224),\n",
       " ('Kremlin', 0.6166538000106812),\n",
       " ('President_Dmitry_Medvedev', 0.6108168959617615),\n",
       " ('President_Barack_Obama', 0.5944156646728516),\n",
       " ('President_Vladimir_Putin', 0.5936282873153687),\n",
       " ('Prime_Minister_Vladimir_Putin', 0.580956220626831),\n",
       " ('Lavrov', 0.5617890357971191),\n",
       " ('Dmitri_Medvedev', 0.5512057542800903),\n",
       " ('Dmitry_Medvedev', 0.5505295991897583)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(positive=['Obama', 'Russia'], negative=['USA']) # Obama + Russia - USA = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mankind', 0.3513610363006592),\n",
       " ('humankind', 0.34724855422973633),\n",
       " ('humanity', 0.3410438299179077),\n",
       " ('macrocosm', 0.3094217777252197),\n",
       " ('intelligence_HUMINT', 0.3089753985404968),\n",
       " ('corporeal', 0.3072369694709778),\n",
       " ('executive_Nancy_Tullos', 0.3065451681613922),\n",
       " ('multiplicities', 0.30226701498031616),\n",
       " ('Christine_Gaugler_head', 0.30204135179519653),\n",
       " ('perfections', 0.30177491903305054)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(positive=['human'], negative=['animal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('unmelted', 0.509214460849762),\n",
       " ('Methane_hydrate', 0.46418970823287964),\n",
       " ('Francies_tossed', 0.45730510354042053),\n",
       " ('ice_crystals', 0.45635735988616943),\n",
       " ('starch_granules', 0.44293591380119324),\n",
       " ('Fill_cocktail_shaker', 0.43867558240890503),\n",
       " ('graphene_sheet', 0.4353669285774231),\n",
       " ('jellylike', 0.4344847798347473),\n",
       " ('ice_cubes', 0.43398386240005493),\n",
       " ('caked_oak_tree', 0.4337309002876282)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(positive=['liquid', 'ice'], negative=['Water']) # water:ice :: liquid: ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What exactly is the Methane Hydrate ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sushi', 0.5657287836074829),\n",
       " ('yakiniku', 0.5292825102806091),\n",
       " ('Teriyaki', 0.5238471031188965),\n",
       " ('Pizzeria', 0.5128323435783386),\n",
       " ('Steak_House', 0.5030418634414673),\n",
       " ('Deli', 0.4990933835506439),\n",
       " ('Grill', 0.49318942427635193),\n",
       " ('sushi', 0.4874907433986664),\n",
       " ('Yakitori', 0.48361146450042725),\n",
       " ('conveyor_belt_sushi', 0.479915976524353)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(positive=['Japan','Pizza'], negative=['USA']) # USA : Pizza :: Japan : ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try word distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To try\n",
    "w1 = \"happy\"\n",
    "w2 = \"cheerful\"\n",
    "w3 = \"sad\"\n",
    "w1_w2_dist = wv.distance(w1, w2)\n",
    "w1_w3_dist = wv.distance(w1, w3)\n",
    "print(\"Synonyms {}, {} have cosine distance: {}\".format(w1, w2, w1_w2_dist))\n",
    "print(\"Antonyms {}, {} have cosine distance: {}\".format(w1, w3, w1_w3_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is an interesting side effect. \n",
    "* Most probably \"happy\" and \"sad\" occur (in the corpus) near each other in sentences, more than \"happy\" and \"cheerful\". \n",
    "* Also, because happy and cheerful are very close, probably most of the sentences use happy rather than cheerful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But the vectors do not have context. \n",
    "###  They will represent \"bank\" in bank (account) & (river) bank with the same vectors\n",
    "### Can we do better ?\n",
    "\n",
    "### Transformers - BERT and GPT-2 will do a much better job in capturing context oin representations\n",
    "### The models also use a different tokenizer, which results inb better contextualization \n",
    "### as well much less vocabulary size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subword tokens\n",
    "* Subword tokens (or word pieces) can be used to split words into multiple pieces, therefore, reducing the vocabulary size for covering every word\n",
    "\n",
    "Vocab Sizes\n",
    "\n",
    "BERT = 30,522 // model dimensionality = 768\n",
    "\n",
    "GPT-2 = 50,257 // model dimensionality = 768 (?)\n",
    "\n",
    "gensym (word2vec-google-news-300) = 3,000,000 // model dimensionality = 300\n",
    "\n",
    "spacy (word2Vec) = 1,340,242 // model dimensionality = 300\n",
    "\n",
    "BERT uses WordPiece tokens, where the non-word-initial pieces start with ##.\n",
    "\n",
    "GPT2, RoBERTa use the BPE (Byte-Pair Encoding), \\u0120 as the special signaling character\n",
    "\n",
    "Ref : https://towardsdatascience.com/comparing-transformer-tokenizers-686307856955"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us use the BERT and GPT models to explore the tokenization & representation\n",
    "#### In later hands-on work, we will use them for NLP tasks like Sentiment analysis, NLG and Question Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0221 20:21:51.401042 4611687872 file_utils.py:41] PyTorch version 1.4.0 available.\n",
      "/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "I0221 20:21:54.472434 4611687872 file_utils.py:57] TensorFlow version 2.0.0-beta1 available.\n"
     ]
    }
   ],
   "source": [
    "# Huggingface Transformers\n",
    "# pip install transformers\n",
    "# Might need rust compiler\n",
    "# curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n",
    "# need Cargo's bin directory ($HOME/.cargo/bin) in your PATH environment variable\n",
    "import transformers\n",
    "# The GPT2 Model transformer with a language modeling head on top \n",
    "#   (linear layer with weights tied to the input embeddings).\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0221 20:21:55.834503 4611687872 tokenization_utils.py:484] loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /Users/ksankar/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "I0221 20:21:55.835951 4611687872 tokenization_utils.py:484] loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /Users/ksankar/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0221 20:21:56.709008 4611687872 configuration_utils.py:254] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /Users/ksankar/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.699bbd1c449e9861456f359d6daa51bd523ac085b4b531ab0aad5a55d091e942\n",
      "I0221 20:21:56.711572 4611687872 configuration_utils.py:290] Model config GPT2Config {\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "I0221 20:21:57.105167 4611687872 modeling_utils.py:458] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /Users/ksankar/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n"
     ]
    }
   ],
   "source": [
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Model config GPT2Config {\n",
    "  \"architectures\": [\n",
    "    \"GPT2LMHeadModel\"\n",
    "  ],\n",
    "  \"attn_pdrop\": 0.1,\n",
    "  \"bos_token_id\": 0,\n",
    "  \"do_sample\": false,\n",
    "  \"embd_pdrop\": 0.1,\n",
    "  \"eos_token_ids\": 0,\n",
    "  \"finetuning_task\": null,\n",
    "  \"id2label\": {\n",
    "    \"0\": \"LABEL_0\",\n",
    "    \"1\": \"LABEL_1\"\n",
    "  },\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"is_decoder\": false,\n",
    "  \"label2id\": {\n",
    "    \"LABEL_0\": 0,\n",
    "    \"LABEL_1\": 1\n",
    "  },\n",
    "  \"layer_norm_epsilon\": 1e-05,\n",
    "  \"length_penalty\": 1.0,\n",
    "  \"max_length\": 20,\n",
    "  \"model_type\": \"gpt2\",\n",
    "  \"n_ctx\": 1024,\n",
    "  \"n_embd\": 768,\n",
    "  \"n_head\": 12,\n",
    "  \"n_layer\": 12,\n",
    "  \"n_positions\": 1024,\n",
    "  \"num_beams\": 1,\n",
    "  \"num_labels\": 2,\n",
    "  \"num_return_sequences\": 1,\n",
    "  \"output_attentions\": false,\n",
    "  \"output_hidden_states\": false,\n",
    "  \"output_past\": true,\n",
    "  \"pad_token_id\": 0,\n",
    "  \"pruned_heads\": {},\n",
    "  \"repetition_penalty\": 1.0,\n",
    "  \"resid_pdrop\": 0.1,\n",
    "  \"summary_activation\": null,\n",
    "  \"summary_first_dropout\": 0.1,\n",
    "  \"summary_proj_to_labels\": true,\n",
    "  \"summary_type\": \"cls_index\",\n",
    "  \"summary_use_proj\": true,\n",
    "  \"temperature\": 1.0,\n",
    "  \"top_k\": 50,\n",
    "  \"top_p\": 1.0,\n",
    "  \"torchscript\": false,\n",
    "  \"use_bfloat16\": false,\n",
    "  \"vocab_size\": 50257\n",
    "}\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n",
      "['We', 'Ġlike', 'ĠUnic', 'orns', 'Ġbecause', 'Ġthey']\n",
      "['Here', 'Ġis', 'Ġthe', 'Ġsentence', 'ĠI', 'Ġwant', 'Ġembed', 'd', 'ings', 'Ġfor', '.']\n"
     ]
    }
   ],
   "source": [
    "print(gpt2_tokenizer.vocab_size)\n",
    "input_text = gpt2_tokenizer.encode(\"We like Unicorns because they\")\n",
    "print(gpt2_tokenizer.convert_ids_to_tokens(input_text))\n",
    "input_text = gpt2_tokenizer.encode(\"Here is the sentence I want embeddings for.\")\n",
    "print(gpt2_tokenizer.convert_ids_to_tokens(input_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can see the BPE and the \\u0120 as the special signalling character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0221 20:23:33.649027 4611687872 tokenization_utils.py:484] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/ksankar/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0221 20:23:34.148659 4611687872 configuration_utils.py:254] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /Users/ksankar/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0221 20:23:34.150888 4611687872 configuration_utils.py:290] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0221 20:23:34.579176 4611687872 modeling_utils.py:458] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /Users/ksankar/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
     ]
    }
   ],
   "source": [
    "# Let us try how BERT does tokenization\n",
    "from transformers import BertTokenizer, BertModel\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n",
      "['[CLS]', 'we', 'like', 'unicorn', '##s', 'because', 'they', '[SEP]']\n",
      "['[CLS]', 'here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.', '[SEP]']\n",
      "['[CLS]', 'i', 'don', \"'\", 't', 'know', 'if', 'it', 'is', 'an', 'em', '##bed', '##ding', ',', 'em', '##bed', '##dable', 'or', 'can', 'be', 'embedded', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(bert_tokenizer.vocab_size)\n",
    "input_text = bert_tokenizer.encode(\"We like Unicorns because they\")\n",
    "print(bert_tokenizer.convert_ids_to_tokens(input_text))\n",
    "input_text = bert_tokenizer.encode(\"Here is the sentence I want embeddings for.\")\n",
    "print(bert_tokenizer.convert_ids_to_tokens(input_text))\n",
    "input_text = bert_tokenizer.encode(\"I don't know if it is an embedding, embeddable or can be embedded\")\n",
    "print(bert_tokenizer.convert_ids_to_tokens(input_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can save the BERT vocabulary to a file and inspect it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n",
    "\n",
    "with open(\"bert_vocabulary.txt\", 'w') as f:\n",
    "    # For each token...\n",
    "    for token in bert_tokenizer.vocab.keys():\n",
    "        # Write it out and escape any unicode characters.            \n",
    "        f.write(token + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interesting to see the character tokens\n",
    "# Ref: Inspect BERT Vocabulary.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_chars = []\n",
    "one_chars_hashes = []\n",
    "\n",
    "# For each token in the vocabulary...\n",
    "for token in bert_tokenizer.vocab.keys():\n",
    "    \n",
    "    # Record any single-character tokens.\n",
    "    if len(token) == 1:\n",
    "        one_chars.append(token)\n",
    "    \n",
    "    # Record single-character tokens preceded by the two hashes.    \n",
    "    elif len(token) == 3 and token[0:2] == '##':\n",
    "        one_chars_hashes.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of single character tokens: 997 \n",
      "\n",
      "! \" # $ % & ' ( ) * + , - . / 0 1 2 3 4 5 6 7 8 9 : ; < = > ? @ [ \\ ] ^ _ ` a b\n",
      "c d e f g h i j k l m n o p q r s t u v w x y z { | } ~ ¡ ¢ £ ¤ ¥ ¦ § ¨ © ª « ¬\n",
      "® ° ± ² ³ ´ µ ¶ · ¹ º » ¼ ½ ¾ ¿ × ß æ ð ÷ ø þ đ ħ ı ł ŋ œ ƒ ɐ ɑ ɒ ɔ ɕ ə ɛ ɡ ɣ ɨ\n",
      "ɪ ɫ ɬ ɯ ɲ ɴ ɹ ɾ ʀ ʁ ʂ ʃ ʉ ʊ ʋ ʌ ʎ ʐ ʑ ʒ ʔ ʰ ʲ ʳ ʷ ʸ ʻ ʼ ʾ ʿ ˈ ː ˡ ˢ ˣ ˤ α β γ δ\n",
      "ε ζ η θ ι κ λ μ ν ξ ο π ρ ς σ τ υ φ χ ψ ω а б в г д е ж з и к л м н о п р с т у\n",
      "ф х ц ч ш щ ъ ы ь э ю я ђ є і ј љ њ ћ ӏ ա բ գ դ ե թ ի լ կ հ մ յ ն ո պ ս վ տ ր ւ\n",
      "ք ־ א ב ג ד ה ו ז ח ט י ך כ ל ם מ ן נ ס ע ף פ ץ צ ק ר ש ת ، ء ا ب ة ت ث ج ح خ د\n",
      "ذ ر ز س ش ص ض ط ظ ع غ ـ ف ق ك ل م ن ه و ى ي ٹ پ چ ک گ ں ھ ہ ی ے अ आ उ ए क ख ग च\n",
      "ज ट ड ण त थ द ध न प ब भ म य र ल व श ष स ह ा ि ी ो । ॥ ং অ আ ই উ এ ও ক খ গ চ ছ জ\n",
      "ট ড ণ ত থ দ ধ ন প ব ভ ম য র ল শ ষ স হ া ি ী ে க ச ட த ந ன ப ம ய ர ல ள வ ா ி ு ே\n",
      "ை ನ ರ ಾ ක ය ර ල ව ා ก ง ต ท น พ ม ย ร ล ว ส อ า เ ་ ། ག ང ད ན པ བ མ འ ར ལ ས မ ა\n",
      "ბ გ დ ე ვ თ ი კ ლ მ ნ ო რ ს ტ უ ᄀ ᄂ ᄃ ᄅ ᄆ ᄇ ᄉ ᄊ ᄋ ᄌ ᄎ ᄏ ᄐ ᄑ ᄒ ᅡ ᅢ ᅥ ᅦ ᅧ ᅩ ᅪ ᅭ ᅮ\n",
      "ᅯ ᅲ ᅳ ᅴ ᅵ ᆨ ᆫ ᆯ ᆷ ᆸ ᆼ ᴬ ᴮ ᴰ ᴵ ᴺ ᵀ ᵃ ᵇ ᵈ ᵉ ᵍ ᵏ ᵐ ᵒ ᵖ ᵗ ᵘ ᵢ ᵣ ᵤ ᵥ ᶜ ᶠ ‐ ‑ ‒ – — ―\n",
      "‖ ‘ ’ ‚ “ ” „ † ‡ • … ‰ ′ ″ › ‿ ⁄ ⁰ ⁱ ⁴ ⁵ ⁶ ⁷ ⁸ ⁹ ⁺ ⁻ ⁿ ₀ ₁ ₂ ₃ ₄ ₅ ₆ ₇ ₈ ₉ ₊ ₍\n",
      "₎ ₐ ₑ ₒ ₓ ₕ ₖ ₗ ₘ ₙ ₚ ₛ ₜ ₤ ₩ € ₱ ₹ ℓ № ℝ ™ ⅓ ⅔ ← ↑ → ↓ ↔ ↦ ⇄ ⇌ ⇒ ∂ ∅ ∆ ∇ ∈ − ∗\n",
      "∘ √ ∞ ∧ ∨ ∩ ∪ ≈ ≡ ≤ ≥ ⊂ ⊆ ⊕ ⊗ ⋅ ─ │ ■ ▪ ● ★ ☆ ☉ ♠ ♣ ♥ ♦ ♭ ♯ ⟨ ⟩ ⱼ ⺩ ⺼ ⽥ 、 。 〈 〉\n",
      "《 》 「 」 『 』 〜 あ い う え お か き く け こ さ し す せ そ た ち っ つ て と な に ぬ ね の は ひ ふ へ ほ ま み\n",
      "む め も や ゆ よ ら り る れ ろ を ん ァ ア ィ イ ウ ェ エ オ カ キ ク ケ コ サ シ ス セ タ チ ッ ツ テ ト ナ ニ ノ ハ\n",
      "ヒ フ ヘ ホ マ ミ ム メ モ ャ ュ ョ ラ リ ル レ ロ ワ ン ・ ー 一 三 上 下 不 世 中 主 久 之 也 事 二 五 井 京 人 亻 仁\n",
      "介 代 仮 伊 会 佐 侍 保 信 健 元 光 八 公 内 出 分 前 劉 力 加 勝 北 区 十 千 南 博 原 口 古 史 司 合 吉 同 名 和 囗 四\n",
      "国 國 土 地 坂 城 堂 場 士 夏 外 大 天 太 夫 奈 女 子 学 宀 宇 安 宗 定 宣 宮 家 宿 寺 將 小 尚 山 岡 島 崎 川 州 巿 帝\n",
      "平 年 幸 广 弘 張 彳 後 御 德 心 忄 志 忠 愛 成 我 戦 戸 手 扌 政 文 新 方 日 明 星 春 昭 智 曲 書 月 有 朝 木 本 李 村\n",
      "東 松 林 森 楊 樹 橋 歌 止 正 武 比 氏 民 水 氵 氷 永 江 沢 河 治 法 海 清 漢 瀬 火 版 犬 王 生 田 男 疒 発 白 的 皇 目\n",
      "相 省 真 石 示 社 神 福 禾 秀 秋 空 立 章 竹 糹 美 義 耳 良 艹 花 英 華 葉 藤 行 街 西 見 訁 語 谷 貝 貴 車 軍 辶 道 郎\n",
      "郡 部 都 里 野 金 鈴 镇 長 門 間 阝 阿 陳 陽 雄 青 面 風 食 香 馬 高 龍 龸 ﬁ ﬂ ！ （ ） ， － ． ／ ： ？ ～\n"
     ]
    }
   ],
   "source": [
    "print('Number of single character tokens:', len(one_chars), '\\n')\n",
    "\n",
    "# Print all of the single characters, 40 per row.\n",
    "\n",
    "# For every batch of 40 tokens...\n",
    "for i in range(0, len(one_chars), 40):\n",
    "    \n",
    "    # Limit the end index so we don't go past the end of the list.\n",
    "    end = min(i + 40, len(one_chars) + 1)\n",
    "    \n",
    "    # Print out the tokens, separated by a space.\n",
    "    print(' '.join(one_chars[i:end]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knight',\n",
       " 'lap',\n",
       " 'survey',\n",
       " 'ma',\n",
       " '##ow',\n",
       " 'noise',\n",
       " 'billy',\n",
       " '##ium',\n",
       " 'shooting',\n",
       " 'guide',\n",
       " 'bedroom',\n",
       " 'priest',\n",
       " 'resistance',\n",
       " 'motor',\n",
       " 'homes',\n",
       " 'sounded',\n",
       " 'giant',\n",
       " '##mer',\n",
       " '150',\n",
       " 'scenes']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bert_tokenizer.vocab.keys())[5000:5020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us follow a text through the BERT model\n",
    "# Ref : https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_text = \"After robbing the bank vault, the bank robber was seen fishing near the river bank\"\n",
    "bank_text = \"After robbing the bank vault, the bank robber was seen fishing on the Mississippi river bank\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'after', 'robb', '##ing', 'the', 'bank', 'vault', ',', 'the', 'bank', 'robber', 'was', 'seen', 'fishing', 'on', 'the', 'mississippi', 'river', 'bank', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "encoded_text = bert_tokenizer.encode(bank_text)\n",
    "print(bert_tokenizer.convert_ids_to_tokens(encoded_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2044, 26211, 2075, 1996, 2924, 11632, 1010, 1996, 2924, 27307, 2001, 2464, 5645, 2006, 1996, 5900, 2314, 2924, 102]\n",
      "tensor([[  101,  2044, 26211,  2075,  1996,  2924, 11632,  1010,  1996,  2924,\n",
      "         27307,  2001,  2464,  5645,  2006,  1996,  5900,  2314,  2924,   102]])\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "torch.Size([1, 20]) torch.Size([1, 20])\n",
      "torch.Size([1, 20, 768])\n"
     ]
    }
   ],
   "source": [
    "encoded_tensor = torch.tensor(encoded_text).unsqueeze(0)\n",
    "segment_ids = [1] * len(encoded_text)\n",
    "print(encoded_text)\n",
    "print(encoded_tensor)\n",
    "print(segment_ids)\n",
    "segment_tensor = torch.tensor([segment_ids]) #.squeeze(0)\n",
    "print(segment_tensor)\n",
    "print(encoded_tensor.size(), segment_tensor.size())\n",
    "bert_model.eval()\n",
    "with torch.no_grad():\n",
    "    encoded_layers, _ = bert_model(encoded_tensor,segment_tensor)\n",
    "print(encoded_layers.size()) # 1 row, 19 tokens, 768 vector dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (101, '[CLS]')\n",
      "1 (2044, 'after')\n",
      "2 (26211, 'robb')\n",
      "3 (2075, '##ing')\n",
      "4 (1996, 'the')\n",
      "5 (2924, 'bank')\n",
      "6 (11632, 'vault')\n",
      "7 (1010, ',')\n",
      "8 (1996, 'the')\n",
      "9 (2924, 'bank')\n",
      "10 (27307, 'robber')\n",
      "11 (2001, 'was')\n",
      "12 (2464, 'seen')\n",
      "13 (5645, 'fishing')\n",
      "14 (2006, 'on')\n",
      "15 (1996, 'the')\n",
      "16 (5900, 'mississippi')\n",
      "17 (2314, 'river')\n",
      "18 (2924, 'bank')\n",
      "19 (102, '[SEP]')\n"
     ]
    }
   ],
   "source": [
    "for i, token_str in enumerate(zip(encoded_text,bert_tokenizer.convert_ids_to_tokens(encoded_text))):\n",
    "  print (i, token_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 vector values for each instance of \"bank\".\n",
      "\n",
      "bank vault    tensor([ 0.8436, -0.4816, -0.0840,  0.4035,  0.6408])\n",
      "bank robber   tensor([ 0.8196, -0.4100, -0.1249,  0.3517,  0.5315])\n",
      "river bank    tensor([-0.3711, -0.6972, -0.6805, -0.1639,  0.4114])\n"
     ]
    }
   ],
   "source": [
    "print('First 5 vector values for each instance of \"bank\".')\n",
    "print('')\n",
    "print(\"bank vault   \", encoded_layers[0][5][:5])\n",
    "print(\"bank robber  \", encoded_layers[0][9][:5])\n",
    "print(\"river bank   \", encoded_layers[0][18][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The vectors are different, for word2vec they would have been the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us do some contextual semantic computations - similarity between words in different and same contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector similarity for  *similar*  meanings [\"bank (robber)\" vs \"bank (vault)\"] :  0.95\n",
      "Vector similarity for  *similar*  meanings [\"robb\" vs \"robber\"]                :  0.70\n",
      "\n",
      "Vector similarity for *different* meanings [\"bank (robber)\" vs \"(river) bank\"] :  0.40\n",
      "Vector similarity for *different* meanings [\"bank (vault)\" vs \"(river) bank\"]  :  0.40\n",
      "\n",
      "Vector similarity for  *similar*  meanings [\"fishing\" vs \"river\"]              :  0.43\n",
      "\n",
      "Vector similarity for *different* meanings [\"fishing\" vs \"bank (vault)\"]       :  0.27\n",
      "Vector similarity for *different* meanings [\"fishing\" vs \"(river) bank\"]       :  0.43\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Calculate the cosine similarity between the word bank \n",
    "# in \"bank robber\" vs \"river bank\" (different meanings).\n",
    "diff_bank_1 = 1 - cosine(encoded_layers[0][9], encoded_layers[0][18])\n",
    "# in \"bank vault\" vs \"river bank\" (different meanings).\n",
    "diff_bank_2 = 1 - cosine(encoded_layers[0][5], encoded_layers[0][18])\n",
    "\n",
    "# Calculate the cosine similarity between the word bank\n",
    "# in \"bank robber\" vs \"bank vault\" (same meaning).\n",
    "same_bank_1 = 1 - cosine(encoded_layers[0][5], encoded_layers[0][9])\n",
    "# in \"robb\" vs \"robber\" (similar meaning).\n",
    "same_bank_2 = 1 - cosine(encoded_layers[0][2], encoded_layers[0][10])\n",
    "\n",
    "# in \"fishing\" vs \"river\" (similar meaning)\n",
    "fishing_river = 1 - cosine(encoded_layers[0][13], encoded_layers[0][17])\n",
    "# in \"fishing\" vs \"bank (vault)\" (different meaning)\n",
    "fishing_bank_v = 1 - cosine(encoded_layers[0][13], encoded_layers[0][5])\n",
    "# in \"fishing\" vs \"(river) bank \" (different meaning)\n",
    "fishing_bank_r = 1 - cosine(encoded_layers[0][13], encoded_layers[0][17])\n",
    "\n",
    "print('Vector similarity for  *similar*  meanings [\"bank (robber)\" vs \"bank (vault)\"] :  %.2f' % same_bank_1)\n",
    "print('Vector similarity for  *similar*  meanings [\"robb\" vs \"robber\"]                :  %.2f' % same_bank_2)\n",
    "print()\n",
    "print('Vector similarity for *different* meanings [\"bank (robber)\" vs \"(river) bank\"] :  %.2f' % diff_bank_1)\n",
    "print('Vector similarity for *different* meanings [\"bank (vault)\" vs \"(river) bank\"]  :  %.2f' % diff_bank_2)\n",
    "print()\n",
    "print('Vector similarity for  *similar*  meanings [\"fishing\" vs \"river\"]              :  %.2f' % fishing_river)\n",
    "print()\n",
    "print('Vector similarity for *different* meanings [\"fishing\" vs \"bank (vault)\"]       :  %.2f' % fishing_bank_v)\n",
    "print('Vector similarity for *similar* meanings [\"fishing\" vs \"(river) bank\"]         :  %.2f' % fishing_bank_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word semantics Still hold, but more contextually\n",
    "* \"bank (robber)\" vs \"bank (vault)\" are very similar\n",
    "* \"bank (robber)\" vs \"(river) bank\" or \"bank (vault)\" vs \"(river) bank\" are less similar\n",
    "* Interestingly \"fishing\" vs \"river\"  and \"fishing\" vs \"(river) bank\" have very similar scores\n",
    "* While \"fishing\" vs \"bank (vault)\" is dissimilar, \"fishing\" vs \"(river) bank\" has some similarity\n",
    "##### All in all, BERT captures the context very well in word representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WIP area\n",
    "### To experiment with different things "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* summary() doesn't work well\n",
    "* you can print(model) to get an idea about the layers\n",
    "* The model_describe() below works reasonably, but the total parameters doesn't look correct\n",
    "* Let me know if you were able to get this to work correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(gpt2_model,[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_describe(model):\n",
    "    print(\"model_summary\")\n",
    "    print()\n",
    "    print(F'Layer_name {\"\":30s} Size {\"\":25s} Number of Parameters')\n",
    "    print(\"=\"*100)\n",
    "    total_params = 0\n",
    "    for param_tensor in model.state_dict():\n",
    "        print(F'Layer {param_tensor:35s} {str(model.state_dict()[param_tensor].size()):30s} '\n",
    "              F'elements : {torch.numel(model.state_dict()[param_tensor]):,}')\n",
    "        total_params += torch.numel(model.state_dict()[param_tensor])\n",
    "    # total_params = sum(p.numel() for p in model.parameters()) # sum number of elements\n",
    "    print(\"=\"*100)\n",
    "    print(f\"Total Params:{total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_summary\n",
      "\n",
      "Layer_name                                Size                           Number of Parameters\n",
      "====================================================================================================\n",
      "Layer embeddings.word_embeddings.weight   torch.Size([30522, 768])       elements : 23,440,896\n",
      "Layer embeddings.position_embeddings.weight torch.Size([512, 768])         elements : 393,216\n",
      "Layer embeddings.token_type_embeddings.weight torch.Size([2, 768])           elements : 1,536\n",
      "Layer embeddings.LayerNorm.weight         torch.Size([768])              elements : 768\n",
      "Layer embeddings.LayerNorm.bias           torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.0.attention.self.query.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.0.attention.self.query.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.0.attention.self.key.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.0.attention.self.key.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.0.attention.self.value.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.0.attention.self.value.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.0.attention.output.dense.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])        elements : 2,359,296\n",
      "Layer encoder.layer.0.intermediate.dense.bias torch.Size([3072])             elements : 3,072\n",
      "Layer encoder.layer.0.output.dense.weight torch.Size([768, 3072])        elements : 2,359,296\n",
      "Layer encoder.layer.0.output.dense.bias   torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.0.output.LayerNorm.weight torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.0.output.LayerNorm.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.1.attention.self.query.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.1.attention.self.query.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.1.attention.self.key.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.1.attention.self.key.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.1.attention.self.value.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.1.attention.self.value.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.1.attention.output.dense.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])        elements : 2,359,296\n",
      "Layer encoder.layer.1.intermediate.dense.bias torch.Size([3072])             elements : 3,072\n",
      "Layer encoder.layer.1.output.dense.weight torch.Size([768, 3072])        elements : 2,359,296\n",
      "Layer encoder.layer.1.output.dense.bias   torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.1.output.LayerNorm.weight torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.1.output.LayerNorm.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.2.attention.self.query.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.2.attention.self.query.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.2.attention.self.key.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.2.attention.self.key.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.2.attention.self.value.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.2.attention.self.value.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.2.attention.output.dense.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])        elements : 2,359,296\n",
      "Layer encoder.layer.2.intermediate.dense.bias torch.Size([3072])             elements : 3,072\n",
      "Layer encoder.layer.2.output.dense.weight torch.Size([768, 3072])        elements : 2,359,296\n",
      "Layer encoder.layer.2.output.dense.bias   torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.2.output.LayerNorm.weight torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.2.output.LayerNorm.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.3.attention.self.query.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.3.attention.self.query.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.3.attention.self.key.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.3.attention.self.key.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.3.attention.self.value.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.3.attention.self.value.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.3.attention.output.dense.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])        elements : 2,359,296\n",
      "Layer encoder.layer.3.intermediate.dense.bias torch.Size([3072])             elements : 3,072\n",
      "Layer encoder.layer.3.output.dense.weight torch.Size([768, 3072])        elements : 2,359,296\n",
      "Layer encoder.layer.3.output.dense.bias   torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.3.output.LayerNorm.weight torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.3.output.LayerNorm.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.4.attention.self.query.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.4.attention.self.query.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.4.attention.self.key.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.4.attention.self.key.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.4.attention.self.value.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.4.attention.self.value.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.4.attention.output.dense.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])        elements : 2,359,296\n",
      "Layer encoder.layer.4.intermediate.dense.bias torch.Size([3072])             elements : 3,072\n",
      "Layer encoder.layer.4.output.dense.weight torch.Size([768, 3072])        elements : 2,359,296\n",
      "Layer encoder.layer.4.output.dense.bias   torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.4.output.LayerNorm.weight torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.4.output.LayerNorm.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.5.attention.self.query.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.5.attention.self.query.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.5.attention.self.key.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.5.attention.self.key.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.5.attention.self.value.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.5.attention.self.value.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.5.attention.output.dense.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])        elements : 2,359,296\n",
      "Layer encoder.layer.5.intermediate.dense.bias torch.Size([3072])             elements : 3,072\n",
      "Layer encoder.layer.5.output.dense.weight torch.Size([768, 3072])        elements : 2,359,296\n",
      "Layer encoder.layer.5.output.dense.bias   torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.5.output.LayerNorm.weight torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.5.output.LayerNorm.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.6.attention.self.query.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.6.attention.self.query.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.6.attention.self.key.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.6.attention.self.key.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.6.attention.self.value.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.6.attention.self.value.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.6.attention.output.dense.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])        elements : 2,359,296\n",
      "Layer encoder.layer.6.intermediate.dense.bias torch.Size([3072])             elements : 3,072\n",
      "Layer encoder.layer.6.output.dense.weight torch.Size([768, 3072])        elements : 2,359,296\n",
      "Layer encoder.layer.6.output.dense.bias   torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.6.output.LayerNorm.weight torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.6.output.LayerNorm.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.7.attention.self.query.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.7.attention.self.query.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.7.attention.self.key.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.7.attention.self.key.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.7.attention.self.value.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.7.attention.self.value.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.7.attention.output.dense.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])        elements : 2,359,296\n",
      "Layer encoder.layer.7.intermediate.dense.bias torch.Size([3072])             elements : 3,072\n",
      "Layer encoder.layer.7.output.dense.weight torch.Size([768, 3072])        elements : 2,359,296\n",
      "Layer encoder.layer.7.output.dense.bias   torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.7.output.LayerNorm.weight torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.7.output.LayerNorm.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.8.attention.self.query.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.8.attention.self.query.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.8.attention.self.key.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.8.attention.self.key.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.8.attention.self.value.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.8.attention.self.value.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.8.attention.output.dense.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])        elements : 2,359,296\n",
      "Layer encoder.layer.8.intermediate.dense.bias torch.Size([3072])             elements : 3,072\n",
      "Layer encoder.layer.8.output.dense.weight torch.Size([768, 3072])        elements : 2,359,296\n",
      "Layer encoder.layer.8.output.dense.bias   torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.8.output.LayerNorm.weight torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.8.output.LayerNorm.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.9.attention.self.query.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.9.attention.self.query.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.9.attention.self.key.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.9.attention.self.key.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.9.attention.self.value.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.9.attention.self.value.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.9.attention.output.dense.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])        elements : 2,359,296\n",
      "Layer encoder.layer.9.intermediate.dense.bias torch.Size([3072])             elements : 3,072\n",
      "Layer encoder.layer.9.output.dense.weight torch.Size([768, 3072])        elements : 2,359,296\n",
      "Layer encoder.layer.9.output.dense.bias   torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.9.output.LayerNorm.weight torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.9.output.LayerNorm.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.10.attention.self.query.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.10.attention.self.query.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.10.attention.self.key.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.10.attention.self.key.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.10.attention.self.value.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.10.attention.self.value.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.10.attention.output.dense.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])        elements : 2,359,296\n",
      "Layer encoder.layer.10.intermediate.dense.bias torch.Size([3072])             elements : 3,072\n",
      "Layer encoder.layer.10.output.dense.weight torch.Size([768, 3072])        elements : 2,359,296\n",
      "Layer encoder.layer.10.output.dense.bias  torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.10.output.LayerNorm.weight torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.10.output.LayerNorm.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.11.attention.self.query.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.11.attention.self.query.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.11.attention.self.key.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.11.attention.self.key.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.11.attention.self.value.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.11.attention.self.value.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer encoder.layer.11.attention.output.dense.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])        elements : 2,359,296\n",
      "Layer encoder.layer.11.intermediate.dense.bias torch.Size([3072])             elements : 3,072\n",
      "Layer encoder.layer.11.output.dense.weight torch.Size([768, 3072])        elements : 2,359,296\n",
      "Layer encoder.layer.11.output.dense.bias  torch.Size([768])              elements : 768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer encoder.layer.11.output.LayerNorm.weight torch.Size([768])              elements : 768\n",
      "Layer encoder.layer.11.output.LayerNorm.bias torch.Size([768])              elements : 768\n",
      "Layer pooler.dense.weight                 torch.Size([768, 768])         elements : 589,824\n",
      "Layer pooler.dense.bias                   torch.Size([768])              elements : 768\n",
      "====================================================================================================\n",
      "Total Params:109,482,240\n"
     ]
    }
   ],
   "source": [
    "model_describe(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_summary\n",
      "\n",
      "Layer_name                                Size                           Number of Parameters\n",
      "====================================================================================================\n",
      "Layer transformer.wte.weight              torch.Size([50257, 768])       elements : 38,597,376\n",
      "Layer transformer.wpe.weight              torch.Size([1024, 768])        elements : 786,432\n",
      "Layer transformer.h.0.ln_1.weight         torch.Size([768])              elements : 768\n",
      "Layer transformer.h.0.ln_1.bias           torch.Size([768])              elements : 768\n",
      "Layer transformer.h.0.attn.bias           torch.Size([1, 1, 1024, 1024]) elements : 1,048,576\n",
      "Layer transformer.h.0.attn.c_attn.weight  torch.Size([768, 2304])        elements : 1,769,472\n",
      "Layer transformer.h.0.attn.c_attn.bias    torch.Size([2304])             elements : 2,304\n",
      "Layer transformer.h.0.attn.c_proj.weight  torch.Size([768, 768])         elements : 589,824\n",
      "Layer transformer.h.0.attn.c_proj.bias    torch.Size([768])              elements : 768\n",
      "Layer transformer.h.0.ln_2.weight         torch.Size([768])              elements : 768\n",
      "Layer transformer.h.0.ln_2.bias           torch.Size([768])              elements : 768\n",
      "Layer transformer.h.0.mlp.c_fc.weight     torch.Size([768, 3072])        elements : 2,359,296\n",
      "Layer transformer.h.0.mlp.c_fc.bias       torch.Size([3072])             elements : 3,072\n",
      "Layer transformer.h.0.mlp.c_proj.weight   torch.Size([3072, 768])        elements : 2,359,296\n",
      "Layer transformer.h.0.mlp.c_proj.bias     torch.Size([768])              elements : 768\n",
      "Layer transformer.h.1.ln_1.weight         torch.Size([768])              elements : 768\n",
      "Layer transformer.h.1.ln_1.bias           torch.Size([768])              elements : 768\n",
      "Layer transformer.h.1.attn.bias           torch.Size([1, 1, 1024, 1024]) elements : 1,048,576\n",
      "Layer transformer.h.1.attn.c_attn.weight  torch.Size([768, 2304])        elements : 1,769,472\n",
      "Layer transformer.h.1.attn.c_attn.bias    torch.Size([2304])             elements : 2,304\n",
      "Layer transformer.h.1.attn.c_proj.weight  torch.Size([768, 768])         elements : 589,824\n",
      "Layer transformer.h.1.attn.c_proj.bias    torch.Size([768])              elements : 768\n",
      "Layer transformer.h.1.ln_2.weight         torch.Size([768])              elements : 768\n",
      "Layer transformer.h.1.ln_2.bias           torch.Size([768])              elements : 768\n",
      "Layer transformer.h.1.mlp.c_fc.weight     torch.Size([768, 3072])        elements : 2,359,296\n",
      "Layer transformer.h.1.mlp.c_fc.bias       torch.Size([3072])             elements : 3,072\n",
      "Layer transformer.h.1.mlp.c_proj.weight   torch.Size([3072, 768])        elements : 2,359,296\n",
      "Layer transformer.h.1.mlp.c_proj.bias     torch.Size([768])              elements : 768\n",
      "Layer transformer.h.2.ln_1.weight         torch.Size([768])              elements : 768\n",
      "Layer transformer.h.2.ln_1.bias           torch.Size([768])              elements : 768\n",
      "Layer transformer.h.2.attn.bias           torch.Size([1, 1, 1024, 1024]) elements : 1,048,576\n",
      "Layer transformer.h.2.attn.c_attn.weight  torch.Size([768, 2304])        elements : 1,769,472\n",
      "Layer transformer.h.2.attn.c_attn.bias    torch.Size([2304])             elements : 2,304\n",
      "Layer transformer.h.2.attn.c_proj.weight  torch.Size([768, 768])         elements : 589,824\n",
      "Layer transformer.h.2.attn.c_proj.bias    torch.Size([768])              elements : 768\n",
      "Layer transformer.h.2.ln_2.weight         torch.Size([768])              elements : 768\n",
      "Layer transformer.h.2.ln_2.bias           torch.Size([768])              elements : 768\n",
      "Layer transformer.h.2.mlp.c_fc.weight     torch.Size([768, 3072])        elements : 2,359,296\n",
      "Layer transformer.h.2.mlp.c_fc.bias       torch.Size([3072])             elements : 3,072\n",
      "Layer transformer.h.2.mlp.c_proj.weight   torch.Size([3072, 768])        elements : 2,359,296\n",
      "Layer transformer.h.2.mlp.c_proj.bias     torch.Size([768])              elements : 768\n",
      "Layer transformer.h.3.ln_1.weight         torch.Size([768])              elements : 768\n",
      "Layer transformer.h.3.ln_1.bias           torch.Size([768])              elements : 768\n",
      "Layer transformer.h.3.attn.bias           torch.Size([1, 1, 1024, 1024]) elements : 1,048,576\n",
      "Layer transformer.h.3.attn.c_attn.weight  torch.Size([768, 2304])        elements : 1,769,472\n",
      "Layer transformer.h.3.attn.c_attn.bias    torch.Size([2304])             elements : 2,304\n",
      "Layer transformer.h.3.attn.c_proj.weight  torch.Size([768, 768])         elements : 589,824\n",
      "Layer transformer.h.3.attn.c_proj.bias    torch.Size([768])              elements : 768\n",
      "Layer transformer.h.3.ln_2.weight         torch.Size([768])              elements : 768\n",
      "Layer transformer.h.3.ln_2.bias           torch.Size([768])              elements : 768\n",
      "Layer transformer.h.3.mlp.c_fc.weight     torch.Size([768, 3072])        elements : 2,359,296\n",
      "Layer transformer.h.3.mlp.c_fc.bias       torch.Size([3072])             elements : 3,072\n",
      "Layer transformer.h.3.mlp.c_proj.weight   torch.Size([3072, 768])        elements : 2,359,296\n",
      "Layer transformer.h.3.mlp.c_proj.bias     torch.Size([768])              elements : 768\n",
      "Layer transformer.h.4.ln_1.weight         torch.Size([768])              elements : 768\n",
      "Layer transformer.h.4.ln_1.bias           torch.Size([768])              elements : 768\n",
      "Layer transformer.h.4.attn.bias           torch.Size([1, 1, 1024, 1024]) elements : 1,048,576\n",
      "Layer transformer.h.4.attn.c_attn.weight  torch.Size([768, 2304])        elements : 1,769,472\n",
      "Layer transformer.h.4.attn.c_attn.bias    torch.Size([2304])             elements : 2,304\n",
      "Layer transformer.h.4.attn.c_proj.weight  torch.Size([768, 768])         elements : 589,824\n",
      "Layer transformer.h.4.attn.c_proj.bias    torch.Size([768])              elements : 768\n",
      "Layer transformer.h.4.ln_2.weight         torch.Size([768])              elements : 768\n",
      "Layer transformer.h.4.ln_2.bias           torch.Size([768])              elements : 768\n",
      "Layer transformer.h.4.mlp.c_fc.weight     torch.Size([768, 3072])        elements : 2,359,296\n",
      "Layer transformer.h.4.mlp.c_fc.bias       torch.Size([3072])             elements : 3,072\n",
      "Layer transformer.h.4.mlp.c_proj.weight   torch.Size([3072, 768])        elements : 2,359,296\n",
      "Layer transformer.h.4.mlp.c_proj.bias     torch.Size([768])              elements : 768\n",
      "Layer transformer.h.5.ln_1.weight         torch.Size([768])              elements : 768\n",
      "Layer transformer.h.5.ln_1.bias           torch.Size([768])              elements : 768\n",
      "Layer transformer.h.5.attn.bias           torch.Size([1, 1, 1024, 1024]) elements : 1,048,576\n",
      "Layer transformer.h.5.attn.c_attn.weight  torch.Size([768, 2304])        elements : 1,769,472\n",
      "Layer transformer.h.5.attn.c_attn.bias    torch.Size([2304])             elements : 2,304\n",
      "Layer transformer.h.5.attn.c_proj.weight  torch.Size([768, 768])         elements : 589,824\n",
      "Layer transformer.h.5.attn.c_proj.bias    torch.Size([768])              elements : 768\n",
      "Layer transformer.h.5.ln_2.weight         torch.Size([768])              elements : 768\n",
      "Layer transformer.h.5.ln_2.bias           torch.Size([768])              elements : 768\n",
      "Layer transformer.h.5.mlp.c_fc.weight     torch.Size([768, 3072])        elements : 2,359,296\n",
      "Layer transformer.h.5.mlp.c_fc.bias       torch.Size([3072])             elements : 3,072\n",
      "Layer transformer.h.5.mlp.c_proj.weight   torch.Size([3072, 768])        elements : 2,359,296\n",
      "Layer transformer.h.5.mlp.c_proj.bias     torch.Size([768])              elements : 768\n",
      "Layer transformer.h.6.ln_1.weight         torch.Size([768])              elements : 768\n",
      "Layer transformer.h.6.ln_1.bias           torch.Size([768])              elements : 768\n",
      "Layer transformer.h.6.attn.bias           torch.Size([1, 1, 1024, 1024]) elements : 1,048,576\n",
      "Layer transformer.h.6.attn.c_attn.weight  torch.Size([768, 2304])        elements : 1,769,472\n",
      "Layer transformer.h.6.attn.c_attn.bias    torch.Size([2304])             elements : 2,304\n",
      "Layer transformer.h.6.attn.c_proj.weight  torch.Size([768, 768])         elements : 589,824\n",
      "Layer transformer.h.6.attn.c_proj.bias    torch.Size([768])              elements : 768\n",
      "Layer transformer.h.6.ln_2.weight         torch.Size([768])              elements : 768\n",
      "Layer transformer.h.6.ln_2.bias           torch.Size([768])              elements : 768\n",
      "Layer transformer.h.6.mlp.c_fc.weight     torch.Size([768, 3072])        elements : 2,359,296\n",
      "Layer transformer.h.6.mlp.c_fc.bias       torch.Size([3072])             elements : 3,072\n",
      "Layer transformer.h.6.mlp.c_proj.weight   torch.Size([3072, 768])        elements : 2,359,296\n",
      "Layer transformer.h.6.mlp.c_proj.bias     torch.Size([768])              elements : 768\n",
      "Layer transformer.h.7.ln_1.weight         torch.Size([768])              elements : 768\n",
      "Layer transformer.h.7.ln_1.bias           torch.Size([768])              elements : 768\n",
      "Layer transformer.h.7.attn.bias           torch.Size([1, 1, 1024, 1024]) elements : 1,048,576\n",
      "Layer transformer.h.7.attn.c_attn.weight  torch.Size([768, 2304])        elements : 1,769,472\n",
      "Layer transformer.h.7.attn.c_attn.bias    torch.Size([2304])             elements : 2,304\n",
      "Layer transformer.h.7.attn.c_proj.weight  torch.Size([768, 768])         elements : 589,824\n",
      "Layer transformer.h.7.attn.c_proj.bias    torch.Size([768])              elements : 768\n",
      "Layer transformer.h.7.ln_2.weight         torch.Size([768])              elements : 768\n",
      "Layer transformer.h.7.ln_2.bias           torch.Size([768])              elements : 768\n",
      "Layer transformer.h.7.mlp.c_fc.weight     torch.Size([768, 3072])        elements : 2,359,296\n",
      "Layer transformer.h.7.mlp.c_fc.bias       torch.Size([3072])             elements : 3,072\n",
      "Layer transformer.h.7.mlp.c_proj.weight   torch.Size([3072, 768])        elements : 2,359,296\n",
      "Layer transformer.h.7.mlp.c_proj.bias     torch.Size([768])              elements : 768\n",
      "Layer transformer.h.8.ln_1.weight         torch.Size([768])              elements : 768\n",
      "Layer transformer.h.8.ln_1.bias           torch.Size([768])              elements : 768\n",
      "Layer transformer.h.8.attn.bias           torch.Size([1, 1, 1024, 1024]) elements : 1,048,576\n",
      "Layer transformer.h.8.attn.c_attn.weight  torch.Size([768, 2304])        elements : 1,769,472\n",
      "Layer transformer.h.8.attn.c_attn.bias    torch.Size([2304])             elements : 2,304\n",
      "Layer transformer.h.8.attn.c_proj.weight  torch.Size([768, 768])         elements : 589,824\n",
      "Layer transformer.h.8.attn.c_proj.bias    torch.Size([768])              elements : 768\n",
      "Layer transformer.h.8.ln_2.weight         torch.Size([768])              elements : 768\n",
      "Layer transformer.h.8.ln_2.bias           torch.Size([768])              elements : 768\n",
      "Layer transformer.h.8.mlp.c_fc.weight     torch.Size([768, 3072])        elements : 2,359,296\n",
      "Layer transformer.h.8.mlp.c_fc.bias       torch.Size([3072])             elements : 3,072\n",
      "Layer transformer.h.8.mlp.c_proj.weight   torch.Size([3072, 768])        elements : 2,359,296\n",
      "Layer transformer.h.8.mlp.c_proj.bias     torch.Size([768])              elements : 768\n",
      "Layer transformer.h.9.ln_1.weight         torch.Size([768])              elements : 768\n",
      "Layer transformer.h.9.ln_1.bias           torch.Size([768])              elements : 768\n",
      "Layer transformer.h.9.attn.bias           torch.Size([1, 1, 1024, 1024]) elements : 1,048,576\n",
      "Layer transformer.h.9.attn.c_attn.weight  torch.Size([768, 2304])        elements : 1,769,472\n",
      "Layer transformer.h.9.attn.c_attn.bias    torch.Size([2304])             elements : 2,304\n",
      "Layer transformer.h.9.attn.c_proj.weight  torch.Size([768, 768])         elements : 589,824\n",
      "Layer transformer.h.9.attn.c_proj.bias    torch.Size([768])              elements : 768\n",
      "Layer transformer.h.9.ln_2.weight         torch.Size([768])              elements : 768\n",
      "Layer transformer.h.9.ln_2.bias           torch.Size([768])              elements : 768\n",
      "Layer transformer.h.9.mlp.c_fc.weight     torch.Size([768, 3072])        elements : 2,359,296\n",
      "Layer transformer.h.9.mlp.c_fc.bias       torch.Size([3072])             elements : 3,072\n",
      "Layer transformer.h.9.mlp.c_proj.weight   torch.Size([3072, 768])        elements : 2,359,296\n",
      "Layer transformer.h.9.mlp.c_proj.bias     torch.Size([768])              elements : 768\n",
      "Layer transformer.h.10.ln_1.weight        torch.Size([768])              elements : 768\n",
      "Layer transformer.h.10.ln_1.bias          torch.Size([768])              elements : 768\n",
      "Layer transformer.h.10.attn.bias          torch.Size([1, 1, 1024, 1024]) elements : 1,048,576\n",
      "Layer transformer.h.10.attn.c_attn.weight torch.Size([768, 2304])        elements : 1,769,472\n",
      "Layer transformer.h.10.attn.c_attn.bias   torch.Size([2304])             elements : 2,304\n",
      "Layer transformer.h.10.attn.c_proj.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer transformer.h.10.attn.c_proj.bias   torch.Size([768])              elements : 768\n",
      "Layer transformer.h.10.ln_2.weight        torch.Size([768])              elements : 768\n",
      "Layer transformer.h.10.ln_2.bias          torch.Size([768])              elements : 768\n",
      "Layer transformer.h.10.mlp.c_fc.weight    torch.Size([768, 3072])        elements : 2,359,296\n",
      "Layer transformer.h.10.mlp.c_fc.bias      torch.Size([3072])             elements : 3,072\n",
      "Layer transformer.h.10.mlp.c_proj.weight  torch.Size([3072, 768])        elements : 2,359,296\n",
      "Layer transformer.h.10.mlp.c_proj.bias    torch.Size([768])              elements : 768\n",
      "Layer transformer.h.11.ln_1.weight        torch.Size([768])              elements : 768\n",
      "Layer transformer.h.11.ln_1.bias          torch.Size([768])              elements : 768\n",
      "Layer transformer.h.11.attn.bias          torch.Size([1, 1, 1024, 1024]) elements : 1,048,576\n",
      "Layer transformer.h.11.attn.c_attn.weight torch.Size([768, 2304])        elements : 1,769,472\n",
      "Layer transformer.h.11.attn.c_attn.bias   torch.Size([2304])             elements : 2,304\n",
      "Layer transformer.h.11.attn.c_proj.weight torch.Size([768, 768])         elements : 589,824\n",
      "Layer transformer.h.11.attn.c_proj.bias   torch.Size([768])              elements : 768\n",
      "Layer transformer.h.11.ln_2.weight        torch.Size([768])              elements : 768\n",
      "Layer transformer.h.11.ln_2.bias          torch.Size([768])              elements : 768\n",
      "Layer transformer.h.11.mlp.c_fc.weight    torch.Size([768, 3072])        elements : 2,359,296\n",
      "Layer transformer.h.11.mlp.c_fc.bias      torch.Size([3072])             elements : 3,072\n",
      "Layer transformer.h.11.mlp.c_proj.weight  torch.Size([3072, 768])        elements : 2,359,296\n",
      "Layer transformer.h.11.mlp.c_proj.bias    torch.Size([768])              elements : 768\n",
      "Layer transformer.ln_f.weight             torch.Size([768])              elements : 768\n",
      "Layer transformer.ln_f.bias               torch.Size([768])              elements : 768\n",
      "Layer lm_head.weight                      torch.Size([50257, 768])       elements : 38,597,376\n",
      "====================================================================================================\n",
      "Total Params:175,620,096\n"
     ]
    }
   ],
   "source": [
    "model_describe(gpt2_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0): Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(gpt2_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_summary\n",
      "\n",
      "Layer_name\t\t\t\t\t\t\tNumber of Parameters\n",
      "====================================================================================================\n",
      "\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "GPT2Model(\n",
      "  (wte): Embedding(50257, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0): Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\t\t\t39383808\n",
      "\n",
      "Linear(in_features=768, out_features=50257, bias=False)\t\t\t1536\n",
      "====================================================================================================\n",
      "Total Params:39385344\n"
     ]
    }
   ],
   "source": [
    "def model_summary(model): # This calculates wrong, may be it is correct - need to check\n",
    "  print(\"model_summary\")\n",
    "  print()\n",
    "  print(\"Layer_name\"+\"\\t\"*7+\"Number of Parameters\")\n",
    "  print(\"=\"*100)\n",
    "  model_parameters = [layer for layer in model.parameters() if layer.requires_grad]\n",
    "  layer_name = [child for child in model.children()]\n",
    "  j = 0\n",
    "  total_params = 0\n",
    "  print(\"\\t\"*10)\n",
    "  for i in layer_name:\n",
    "    print()\n",
    "    param = 0\n",
    "    try:\n",
    "      bias = (i.bias is not None)\n",
    "    except:\n",
    "      bias = False  \n",
    "    if not bias:\n",
    "      param =model_parameters[j].numel()+model_parameters[j+1].numel()\n",
    "      j = j+2\n",
    "    else:\n",
    "      param =model_parameters[j].numel()\n",
    "      j = j+1\n",
    "    print(str(i)+\"\\t\"*3+str(param))\n",
    "    total_params+=param\n",
    "  print(\"=\"*100)\n",
    "  print(f\"Total Params:{total_params}\")       \n",
    "\n",
    "model_summary(gpt2_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handy code for testing elapsed time. It prints hh:mm:ss.nnn correctly. time.time() prints the total seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed - 0:02:25.006721\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "start_time = datetime.datetime.now()\n",
    "time.sleep(145)\n",
    "print(F'Elapsed - {datetime.datetime.now() - start_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _That's All Folks !_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
